---
title: "Simulation Study for Biostatistics Practicum"
author: "Zachary Katz (UNI: zak2132)"
date: "2023-01-26"
output: html_notebook
editor_options: 
  chunk_output_type: console
---

```{r}
# Load packages
library(tidyverse)
library(MASS)
library(corrplot)
library(faux)
library(patchwork)
library(glmnet)
library(rsample)
library(caret)
library(earth)
library(BART)
library(ranger)
library(mgcv)
library(SuperLearner)
library(qgcomp)
library(grf)
# library(amc)
# library(remotes)
# library(tmleCommunity) # Still in beta testing
# library(bkmr)

# To avoid annoying message
options(dplyr.summarise.inform = FALSE)
```

# Simulate data

```{r}
# Data simulation function

data_simulation = function(scenario, N = 1000) {
  
  # Scenario 1: baseline (simple exposures, simple confounding)
  if (scenario == 1){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate linear effect from two exposures (M1, M5) and five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
      0.25*z[ind1] + 0.25*z[ind2] + 0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7]
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }

  # Scenario 2: complex exposures (nonlinear / interactions), simple confounding
  if (scenario == 2) {

    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 3: complex exposures (nonlinear / interactions) + multicollinearity, simple confounding
  if (scenario == 3){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # Exposures have higher correlation to each other
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 4: complex exposures (trigonometric), simple confounding
  if (scenario == 4){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 5: complex exposures (trigonometric) + multicollinearity, simple confounding
  if (scenario == 5){
  
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # Exposures have higher correlation to each other
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
      0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
      0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 6: simple exposures, complex confounding
  if (scenario == 6){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate linear effect from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1-5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
  
      0.5*z[ind1] + 0.5*z[ind2] + # Simple exposure effect, omitting C2
      0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
      0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
}
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenarion 7: complex exposures (nonlinear / interactions), complex confounding
  if (scenario == 7){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect (nonlinear/interaction)
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 8: complex exposures (trigonometric), complex confounding
  if (scenario == 8){
  
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 9: complex exposures (nonlinear / interactions) + multicollinearity, complex confounding
  if (scenario == 9){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # High collinearity among the exposures
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect (nonlinear/interaction)
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # 10 = complex exposures (trigonometric) + multicollinearity, complex confounding
  if (scenario == 10){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # High collinearity among the exposures
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }

}

# Check correlations
data = data_simulation(scenario = 10, N = 1000)
corr = cor(data)
corrplot(corr, type = "upper")
```

```{r}
# Function for functional forms

functional_forms = function(scenario, covars){
  
  if(scenario == 1){
    
    a = 0.25*covars[1] + 0.25*covars[5] + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
  }
  
  if(scenario %in% c(2, 3)){
    
    a = 0.2*(covars[1])^2 + 0.2*(covars[5])^2 + 0.1*(covars[1]*covars[5]) + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
  }
  
  if(scenario %in% c(4, 5)){
    
    a = 0.35*sin(covars[1]) + 0.35*sin(covars[5]) + 0.3*(covars[1])*(covars[5]) + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
      
  }
  
  if (scenario == 6){
    
    a = 0.5*covars[1] + 0.5*covars[5] + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])
    
    }
    
  if (scenario %in% c(7, 9)){
    
    a = 0.2*(covars[1])^2 + 0.2*(covars[5])^2 + 0.1*(covars[1]*covars[5]) + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])
      
    }
    
  if (scenario %in% c(8, 10)){
    
    a = 0.35*sin(covars[1]) + 0.35*sin(covars[5]) + 0.3*(covars[1])*(covars[5]) + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])

    }
 
  return(a)   
}
```

```{r}
# First estimand of interest: effect of interquartile range change in exposure levels

quantiles = data.frame()

# For each scenario, simulate 1000 datasets with 1000 observations

for (i in 1:10){
  
  for (j in 1:1000){
    
    sim_data = data_simulation(scenario = i, N = 1000)
    q1 = apply(sim_data, 2, function(x) quantile(x, 0.25))
    med = apply(sim_data, 2, median)
    q2 = apply(sim_data, 2, function(x) quantile(x, 0.75))
    quantiles = bind_rows(
      quantiles,
      bind_cols(scenario = i, quantile = c(0.25, 0.50, 0.75),
      bind_rows(q1, med, q2)
      )
    )
    
  }
  
}

# For each scenario, average across datasets for each quantile of a given metal

# Function for table
metal_IQR_table = function(scenario, metal){
  
  # Average across datasets of each quantile for given metal in given scenario
  m.25 = quantiles %>% 
          filter(quantile == 0.25 & scenario == {{scenario}}) %>% 
          dplyr::select(c(metal + 2)) %>% 
          t() %>% 
          mean()
    
  m.75 = quantiles %>% 
          filter(quantile == 0.75 & scenario == {{scenario}}) %>% 
          dplyr::select(c(metal + 2)) %>% 
          t() %>% 
          mean()
  
  # For interquartile range change
  # Create tables with all vars at median except for given exposure to obtain potential outcomes (counterfactuals)
  q1.m = apply(filter(quantiles, quantile == 0.5 & scenario == {{scenario}}), 2, mean)
  q1.m[[c(metal + 2)]] = m.25
  q2.m = q1.m
  q2.m[[c(metal + 2)]] = m.75
  
  table = rbind(q1.m, q2.m) %>% as.data.frame() %>% dplyr::select(-scenario, -quantile)
  
  return(table)
  
}

# Function for causal effect (difference in potential outcomes)
metal_IQR_effect = function(scenario, metal){
  
  # Obtain metal IQR table
  table = metal_IQR_table(scenario, metal)
  
  # Calculate true effect
  true_effect = functional_forms(scenario, table[2, ]) - functional_forms(scenario, table[1, ])
  
  return(true_effect[[1]])
  
}

# Alternative code for metal IQR table
# metal_IQR = data.frame()
# 
# for (scenario in 1:10){
#   
#   metal_changes = data.frame()
#   
#   for (metal in 1:10){
#     
#     # Average across datasets of each quantile for given metal in given scenario
#     m.25 = quantiles %>% 
#             filter(quantile == 0.25 & scenario == scenario) %>% 
#             dplyr::select(c(metal + 2)) %>% 
#             t() %>% 
#             mean()
#       
#     m.75 = quantiles %>% 
#             filter(quantile == 0.75 & scenario == scenario) %>% 
#             dplyr::select(c(metal + 2)) %>% 
#             t() %>% 
#             mean()
#     
#     # For interquartile range change
#     # Create tables with all vars at median except for given exposure to obtain potential outcomes (counterfactuals)
#     q1.m = apply(filter(quantiles, quantile == 0.5 & scenario == scenario), 2, mean)
#     q1.m[[c(metal + 2)]] = m.25
#     q2.m = q1.m
#     q2.m[[c(metal + 2)]] = m.75
#     
#     table = rbind(q1.m, q2.m) %>% as.data.frame() %>% dplyr::select(-scenario, -quantile)
#     
#     table$scenario = scenario
#     table$metal = metal
#     table$quantile = c(0.25, 0.75)
#     
#     metal_changes = bind_rows(metal_changes, table)
#     
#   }
#   
#   metal_IQR = bind_rows(metal_IQR, metal_changes)
#   
# }

# Alternative code for true effects table
# true_effects_alt = data.frame()
# 
# for (scenario in 1:10){
#   
#   rows = data.frame()
#   
#   for (metal in 1:10){
#     
#     table = metal_IQR_table(scenario, metal)
#     
#     true_effect = (functional_forms(scenario, table[2, ]) - functional_forms(scenario, table[1, ]))[[1]] %>% as.numeric()
#     
#     row = bind_cols(
#       scenario = scenario,
#       metal = metal,
#       effect = true_effect
#     )
#     
#     rows = bind_rows(rows, row)
#     
#   }
#   
#   true_effects_alt = bind_rows(true_effects_alt, rows)
#   
# }
```

```{r}
# Calculate true effects
true_effects = data.frame()

for (scenario_i in 1:10){
  
  for (metal_j in 1:10){
    
    effect = metal_IQR_effect(scenario_i, metal_j)
    true_effects = bind_rows(
      true_effects,
      bind_cols(scenario = scenario_i,
                metal = metal_j,
                effect = effect)
    )
    
  }
  
}

true_effects = true_effects %>% 
  pivot_wider(
    names_from = metal,
    values_from = effect
  )
```

```{r}
# Repeat but for aggregate mixture profiles

# Function for table
metal_IQR_mix_table = function(scenario){
  
  # Average across datasets of each quantile for given metal in given scenario
  m.25 = quantiles %>% 
    filter(quantile == 0.25 & scenario == {{scenario}}) %>% 
    dplyr::select(M1:Y)
    
  m.75 = quantiles %>% 
    filter(quantile == 0.75 & scenario == {{scenario}}) %>% 
    dplyr::select(M1:Y)
  
  q.1.means = apply(m.25, 2, mean)
  q.2.means = apply(m.75, 2, mean)
  
  table = rbind(q.1.means, q.2.means) %>% as.data.frame()
  
  return(table)
  
}

# Function for causal effect with aggregate mixture profile
metal_IQR_mix_effect = function(scenario){
  
  # Obtain metal IQR table
  table = metal_IQR_mix_table(scenario)
  
  # Calculate true effect
  true_effect = functional_forms(scenario, table[2, ]) - functional_forms(scenario, table[1, ])
  
  return(true_effect[[1]])
  
}

# Calculate true effects
true_effects_mixture = data.frame()

for (scenario in 1:10){
    
    effect = metal_IQR_mix_effect(scenario)
    true_effects_mixture = bind_rows(
      true_effects_mixture,
      bind_cols(scenario = scenario,
                effect = effect)
    )
  
}
```

```{r}
# Simulate M data sets of N observations for each scenario
set.seed(2132)
M = 10 # number of data sets per scenario (10 for testing code; 100 or 1000 eventually)
N = 1000 # number of observations per data set

simulated_data = list()

for (scenario in 1:10){
  
  simulated_data_loop = lapply(1:M, function(i) data_simulation(scenario = scenario, N = N))
  
  for (data_frame in 1:M){
    
    simulated_data_loop[[data_frame]]$scenario = scenario
    
  }
  
  simulated_data = append(simulated_data, simulated_data_loop)
  
}
```

# Other functions

```{r}
# Function to bootstrap N times on a given model
boot_fnct = function(model_type, bootstrap_data, boot_count){
  
  set.seed(2132)
  
  boot = bootstrap_data %>% 
    dplyr::select(M1:Y) %>%
    bootstraps(times = boot_count) %>% 
    rowwise() %>% 
    mutate(data_sample = (list(analysis(splits)))) %>% 
    dplyr::select(id, data_sample) %>% 
    ungroup() %>%
    mutate(models = map(data_sample, model_type = model_type, model_fit_fnct)) %>% 
    dplyr::select(-data_sample)
  
  return(boot)
  
}

# Function to estimate causal effect using fitted model
predict_diff_fnct = function(model_type, fitted_model, q1, q2){
  
  if (model_type %in% c("linear","elastic net", "MARS", "GAM", "Random Forest", "GAM2", "Causal Forest")){
    
    predicted = predict(fitted_model, q2) - predict(fitted_model, q1)

  }
  
  else if (model_type %in% c("BART")){
    
    # For BART, SE medians from MC draws; is there a better way?
    predicted = quantile(predict(fitted_model, q2), 0.5) - quantile(predict(fitted_model, q1), 0.5)
    
  }
  
  else if (model_type %in% c("SuperLearner")){
    
    predicted = predict(fitted_model, q2, type = "response")$pred - predict(fitted_model, q1, type = "response")$pred
    
  }
  
  else if (model_type %in% c("Quantile G-Computation", "AMC G-Computation")){
    
    exposures = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10")
    
    predicted = predict(object = fitted_model, expnms = exposures, newdata = q2) - predict(object = fitted_model, expnms = exposures, newdata = q1)
    
  }
  
  else if (model_type %in% c("BKMR1")){
    
    predicted = 
      mean(
        SamplePred(fitted_model,
                     Znew = q2[,variable.names(fitted_model$Z)],
                     Xnew = as.matrix(q2[,variable.names(fitted_model$X)]))
        - 
          SamplePred(fitted_model,
                   Znew = q1[,variable.names(fitted_model$Z)],
                   Xnew = as.matrix(q1[,variable.names(fitted_model$X)]))
                   )
    
  }
  
  else if (model_type %in% c("BKMR2")){
    
    predicted = 
      mean(
        SamplePred(fitted_model,
                     Znew = q2[,variable.names(fitted_model$Z)])
        - 
          SamplePred(fitted_model,
                   Znew = q1[,variable.names(fitted_model$Z)])
                   )
    
  }

  return(predicted)
  
}

# Function to estimate causal effects using fitted model on bootstraps
predict_diff_boots = function(model_type, boots, q1, q2){
  
  bstar = NULL 
  n = dim(boots)[1];
  
  for (mod in 1:n) {
    
    if (model_type == "elastic net"){
      
      quant1 = predict(boots[[2]][[mod]], q1)
      quant1 = quant1[length(quant1)] # Pull for smallest lambda
      quant2 = predict(boots[[2]][[mod]], q2)
      quant2 = quant2[length(quant2)] # Pull for smallest lambda
      diff = tibble(diff = quant2 - quant1)[[1]]
      
    }
    
    else if (model_type == "BART"){
      
      # Use medians; is there a better method?
      quant1 = quantile(predict(boots[[2]][[mod]], q1), 0.5)
      quant2 = quantile(predict(boots[[2]][[mod]], q2), 0.5)
      diff = tibble(diff = quant2 - quant1)[[1]]
      
    }
    
    else {
      
      diff = tibble(diff = predict_diff_fnct(
        model_type,
        fitted_model = boots[[2]][[mod]],
        q1,
        q2)[[1]]
    )
      
    }
    
    bstar = rbind(bstar, diff)
  } # Next draw
  
  return(bstar)
  
}
```

```{r}
# Function for performance metrics for individual metals and mixture of metals
performance_metrics = function(estimates, scenario_i, metal_j){
  
  if (metal_j %in% c(1:10)){
    
    true_effect = true_effects[scenario_i, metal_j + 1]
    
  }
  
  if (metal_j %in% c(11)){
    
    true_effect = true_effects_mixture[scenario_i, ][, 2]
    
  }

  estimates = estimates %>%
    dplyr::select(-dataset) %>%
    filter(scenario == scenario_i & metal == metal_j) %>% # Used to be {{scenario}} {{metal}}
    mutate(
      true_effect = as.numeric(true_effect)
    ) %>%
    mutate(
      point_bias = abs(true_effect - point_estimate),
      standard_error = (true_effect - point_estimate)^2,
      includes = if_else(CI_lower <= true_effect & CI_upper >= true_effect,
                         1,
                         0),
      CI_length = CI_upper - CI_lower,
      sig_effect = case_when(
        CI_lower > 0 & CI_upper > 0 ~ 1,
        CI_lower < 0 & CI_upper < 0 ~ 1,
        CI_lower < 0 & CI_upper > 0 ~ 0
      )
    )

  estimates_grouped = estimates %>%
    group_by(scenario, metal) %>%
    summarize(
      mean_point_estimate = mean(point_estimate),
      true_effect = mean(true_effect),
      bias = mean(abs(point_bias)),
      relative_bias = bias / true_effect,
      variance_point_estimate = var(point_estimate),
      RMSE = sqrt(mean(standard_error)),
      coverage = mean(includes),
      mean_sig_effect = mean(sig_effect)
      ) %>% 
    mutate(
      TPR = dplyr::case_when(
        metal %in% c(1, 5) ~ mean_sig_effect
        ),
      FDR = dplyr::case_when(
        !(metal %in% c(1, 5)) ~ mean_sig_effect
      )
    )
  
  row = estimates_grouped %>%
    dplyr::select(scenario, metal, mean_point_estimate, variance_point_estimate, bias, relative_bias, RMSE, coverage, TPR, FDR) %>%
    head(1)

  return(row)

}

# Loop across models, scenarios, metals
performance = function(estimates){
  
  performance_df = data.frame()

    for (scenario_i in 1:10){
      
      for (metal_j in 1:11){ # Include metal 11 as mixture profile
        
        # Run performance function for individual metals
        row = performance_metrics(estimates, scenario_i, metal_j)
        
        # Append to data frame
        performance_df = bind_rows(performance_df, row)
        
      }
  
    }
  
  return(performance_df)
  
}
```

```{r}
# Function to filter to correct dataset
dataset_filter = function(scenario){
  
  # Filter for scenario and then to each individual dataset for looping
  first_index = (scenario * M) - (M - 1) 
  last_index = first_index + (M - 1)
  indices = seq(first_index, last_index, by = 1)
  filtered_datasets = simulated_data[indices]
  
  return(filtered_datasets)
  
}
```

```{r}
# Performance graphics
perform_graphics = function(model_name, data = all_models_df){
  
  base = data %>% 
    filter(model_type == {{model_name}}) %>% 
    mutate(
      metal = as.factor(metal),
      scenario = as.factor(scenario)
    ) %>% 
    ggplot()
  
  a = base + 
    geom_point(aes(x = metal, y = RMSE, group = scenario, color = scenario))
  
  b = base + 
    geom_point(aes(x = metal, y = bias, group = scenario, color = scenario))
  
  c = base + 
    geom_point(aes(x = metal, y = relative_bias, group = scenario, color = scenario))
  
  d = base + 
    geom_point(aes(x = metal, y = coverage, group = scenario, color = scenario))
  
  e = base + 
    geom_point(aes(x = metal, y = TPR, group = scenario, color = scenario))
  
  f = base + 
    geom_point(aes(x = metal, y = FDR, group = scenario, color = scenario))
  
  # e = data %>%
  #   mutate(
  #     metal = as.factor(metal),
  #     scenario = as.factor(scenario)
  #   ) %>% 
  #   ggplot() + 
  #   geom_bar(aes(y = RMSE, color = model_type, fill = model_type)) + 
  #   facet_wrap(scenario ~ metal)
  
  graphs = (a + b) / (c + d) / (e + f)
  
  return(graphs)
  
}
```

# Test models

```{r}
# Function to fit models
model_fit_fnct = function(model_type, data){
  
  if (model_type %in% c("linear")){
    
    fit = lm(Y ~ ., 
          data = data)
    
  }
  
  else if (model_type %in% c("elastic net")){
    
    x_data = data %>% 
      dplyr::select(-Y) %>% 
      as.matrix()
    
    y_data = data %>% 
      dplyr::select(Y) %>% 
      as.matrix()
  
    # Force covariates into model (specify covariates not to drop; 0 for vars to keep)
    p_fac = rep(1, dim(x_data)[2])
    p_fac[11:15] = 0 # Keep confounders in model
    
    # Vector of values identifying what fold each observation is in for 10-fold cross-validation
    fold_id = sample(x = 1:10, size = length(y_data), replace = TRUE)
    
    # Fit model to training data with cross-validation
    # Alpha = 1 by default
    cv_fit = cv.glmnet(
      x = x_data,
      y = y_data,
      family = "gaussian",
      maxit = 5000,
      penalty.factor = p_fac,
      foldid = fold_id 
    )
    
    # Need to hard code cross-validation over range of alpha values
    for (i in seq(0, 1, by = 0.1)){
      cv_temp = cv.glmnet(
        x = x_data,
        y = y_data,
        family = "gaussian",
        maxit = 5000,
        penalty.factor = p_fac,
        foldid = fold_id,
        alpha = i
      )
      
      if(min(cv_temp$cvm) < min(cv_fit$cvm)) 
        {cv_fit = cv_temp}
    
  }
  
  # Optimal beta coefficients
  selected_beta_coefs = array(t(as.matrix(coef(cv_fit,
                                         s = cv_fit$lambda.min))))
  
  # Final trained model for prediction
  # Returns elastic net model with coefficients across grid of values for regularization parameter lambda
  fit = glmnet(
    x = x_data,
    y = y_data,
    family = "gaussian",
    init = selected_beta_coefs,
    iter = 0
  )
    
  }
  
  else if (model_type %in% c("MARS")){
    
    train_data = data %>% 
      as.matrix()
  
    # Specify confounders to force into the model
    covars = c("C1", "C2", "C3", "C4", "C5") 
    
    # Implement 5-fold CV using caret package
    myControl = trainControl(
      method = "cv",
      number = 5,
      summaryFunction = defaultSummary
    )
    
    # Set hyperparameter tuning grid, may want to consider different values
    hyper_grid = expand.grid(
      degree = c(1,2), # Limit to up to second-order polynomials
      nprune = seq(5, 50, length.out = 10) %>% floor()
    )
    
    # Fit model
    fit = train(
      Y ~ .,
      data = train_data,
      method = "earth",
      linpreds = covars, # Enter covariates linearly
      thresh = 0, # Include a predictor even if it has very little predictive power
      penalty = -1, # Do not discard any terms in backwards pass
      trControl = myControl,
      tuneGrid = hyper_grid
    )
    
  }
  
  else if (model_type %in% c("GAM")){
    
    x_train = data %>% 
      dplyr::select(M1:C5) %>% 
      as.matrix()
  
    y_train = data %>% 
      dplyr::select(Y) %>% 
      as.matrix() %>% 
      as.numeric()
    
    # Cross-validation for smoothing functions
    ctrl = trainControl(method = "cv", number = 10)
    
    # Grid for hyperparameters
    # select = TRUE/FALSE for variable selection using GCV
    grid = data.frame(method = "GCV.Cp", select = c(TRUE,FALSE))
    
    # Fit GAM
    fit = train(
      x = x_train, # By default, GAM adds s() term for predictors having > 10 values, linear for those that do not
      y = y_train,
      method = "gam", # MGCV implementation of GAM
      tuneGrid = grid,
      trControl = ctrl
  )
    
  }
  
  else if (model_type %in% c("BART")){
    
    x_train = data %>%
      dplyr::select(M1:C5) %>%
      as.matrix()

    y_train = data %>%
      dplyr::select(Y) %>%
      as.matrix()

    fit = mc.wbart( # Use mc.wbart for parallel computation (just regular wbart without)
      x.train = x_train,
      y.train = y_train,
      x.test = x_train,
      ntree = 50, # Number of trees in the sum
      nskip = 250, # Number of MCMC iterations to be treated as burn in
      ndpost = 1000, # Number of posterior draws returned
      keepevery = 250,
      seed = 2132,
      mc.cores = 4 # Parallel computing
    )

  }
  
  else if (model_type %in% c("Random Forest")){
    
    train_data = data %>% 
      as.matrix()
  
    ctrl = trainControl(method = "cv") 
    
    rf_grid = expand.grid(
      mtry = 1:10, # of predictors at each split
      splitrule = "variance", # RSS
      min.node.size = 1:6 # Controls size of tree
    )
    
    fit = train(
      Y ~ ., # Assumes no interaction terms?
      data = train_data,
      method = "ranger",
      tuneGrid = rf_grid,
      trControl = ctrl
    )
    
  }
  
  else if (model_type %in% c("GAM2")){
  
  # Fit GAM using MGCV
  fit = mgcv::gam(
    # k = 4 is max polynomial for smoothing basis
    Y ~ ti(M1, k = 4) + ti(M2, k = 4) + ti(M3, k = 4) + ti(M4, k = 4) + ti(M5, k = 4) + ti(M6, k = 4) + ti(M7, k = 4) + ti(M8, k = 4) + ti(M9, k = 4) + ti(M10, k = 4) + 
      # Add two-way interactions for tensor smoothing
      ti(M1, M2, k = 4) + ti(M1, M3, k = 4) + ti(M1, M4, k = 4) + ti(M1, M5, k = 4) + ti(M1, M6, k = 4) + ti(M1, M7, k = 4) + ti(M1, M8, k = 4) + ti(M1, M9, k = 4) + ti(M1, M10, k = 4) + ti(M2, M3, k = 4) + ti(M2, M4, k = 4) + ti(M2, M5, k = 4) + ti(M2, M6, k = 4) + ti(M2, M7, k = 4) + ti(M2, M8, k = 4) + ti(M2, M9, k = 4) + ti(M2, M10, k = 4) + ti(M3, M4, k = 4) + ti(M3, M5, k = 4) + ti(M3, M6, k = 4) + ti(M3, M7, k = 4) + ti(M3, M8, k = 4) + ti(M3, M9, k = 4) + ti(M3, M10, k = 4) + ti(M4, M5, k = 4) + ti(M4, M6, k = 4) + ti(M4, M7, k = 4) + ti(M4, M8, k = 4) + ti(M4, M9, k = 4) + ti(M4, M10, k = 4) + ti(M5, M6, k = 4) + ti(M5, M7, k = 4) + ti(M5, M8, k = 4) + ti(M5, M9, k = 4) + ti(M5, M10, k = 4) + ti(M6, M7, k = 4) + ti(M6, M8, k = 4) + ti(M6, M9, k = 4) + ti(M6, M10, k = 4) + ti(M7, M8, k = 4) + ti(M7, M9, k = 4) + ti(M7, M10, k = 4) + ti(M8, M9, k = 4) + ti(M8, M10, k = 4) + ti(M9, M10, k = 4) + 
      # Add covariates
      C1 + C2 + C3 + C4 + C5,
    data = data,
    method = "REML"
  )
    
  }
  
  else if (model_type %in% c("SuperLearner")){
    
    x_train = data %>% 
      dplyr::select(M1:C5) %>% 
      as.data.frame()
  
    y_train = data %>% 
      dplyr::select(Y) %>% 
      as.matrix() %>% 
      as.numeric()
  
    # Select variety of candidate learners, some parametric, some non-parametric
    SL_library_chosen = c("SL.glmnet", "SL.randomForest", "SL.glm", "SL.ranger", "SL.xgboost", "SL.svm", "SL.earth", "SL.gbm", "SL.ranger")
  
    # Select loss function for meta learner and estimate risk
    # Here, use non-negative least squares loss
    loss_chosen = "method.NNLS"
    
    # Fit WITHOUT cross-validation (try with CV later to improve stability / robustness?)
    fit = SuperLearner(
      Y = y_train,
      X = x_train,
      SL.library = SL_library_chosen,
      family = "gaussian",
      method = loss_chosen,
      verbose = FALSE
    )
    
  }
  
  else if (model_type %in% c("Quantile G-Computation")){
    
    data = data %>% 
      dplyr::select(M1:Y) %>% 
      as.data.frame()
  
    exposures = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10")
    
    # Fit g-computation model with all two-way interactions (including self-interactions, i.e. quadratic terms)
    fit = qgcomp.boot(
      Y ~ . + .^2,
      data = data,
      family = gaussian(),
      expnms = exposures,
      q = 4, # Number of quantiles to create indicators representing exposure vars (can increase if we want)
      B = 200, # Number of bootstrap iterations
      degree = 2, # Polynomial bases for marginal model
      seed = 2132
    )
    
    # https://github.com/alexpkeil1/qgcomp/blob/main/vignettes/qgcomp-vignette.Rmd 
    # Note: Why don't I get weights from the `boot` functions?
# Users often use the `qgcomp.*.boot` functions because the want to marginalize over confounders or fit a non-linear joint exposure function. In both cases, the overall exposure response will no longer correspond to a simple weighted average of model coefficients, so none of the `qgcomp.*.boot` functions will calculate weights. In most use cases, the weights would vary according to which level of joint exposure you're at, so it is not a straightforward proposition to calculate them (and you may not wish to report 4 sets of weights if you use the default `q=4`). If you fit an otherwise linear model, you can get weights from a `qgcomp.*.noboot` which will be very close to the weights you might get from a linear model fit via `qgcomp.*.boot` functions, but be explicit that the weights come from a different model than the inference about joint exposure effects.
    
  }
  
  else if (model_type %in% c("Causal Forest")){
    
    # Causal random forest (`grf`) using regression_forest, which trains a regression forest that can be used to estimate the conditional mean function mu(x) = E[Y | X = x]
    # https://grf-labs.github.io/grf/REFERENCE.html#:~:text=For%20regression%20forests%2C%20the%20prediction,status%20of%20the%20neighbor%20examples.
    # Note: should I use quantile_forest instead?
    
    x_train = data %>% 
      dplyr::select(M1:C5) %>% 
      as.matrix()
  
    y_train = data %>% 
      dplyr::select(Y) %>% 
      as.matrix()
    
    # Fit regression forest model using mostly defaults
    fit = regression_forest(
      X = x_train,
      Y = y_train,
      seed = 2132
      
  )
    
  }
  
  else if (model_type %in% c("AMC G-Computation")){
    
    data = data %>% 
      distinct() # To make it work with bootstrapping (other contrasts issue re: number of levels)
  
    x_train = data %>% 
      dplyr::select(M1:C5)
    
    amc_result = amc(
      data = x_train
    )
    
    output = amc_result$output
    
    oldseed = .Random.seed
    
    output = cbind(data %>% dplyr::select(Y), output)
    
    exposures = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10")
    
    # Obtain joint mixture effect using same method as Quantile-based G-Computation
    fit = qgcomp.boot(
      Y ~ . + .^2,
      data = output,
      family = gaussian(),
      expnms = exposures,
      q = 4, # Number of quantiles to create indicators representing exposure vars (can increase if we want)
      B = 200, # Number of bootstrap iterations
      degree = 2, # Polynomial bases for marginal model
      seed = 2132
    )
    
  }
  
  else if (model_type %in% c("BKMR1")){
    
    data = data %>% 
      distinct()
  
    covars = data %>% 
      dplyr::select(C1:C5) %>% 
      as.matrix()
    
    exposures = data %>% 
      dplyr::select(M1:M10) %>% 
      as.matrix()
    
    y_train = data %>% 
      dplyr::select(Y) %>% 
      as.matrix()
    
    # Use knots to improve speed
    knots100 = fields::cover.design(
      exposures,
      nd = 100
    )$design
    
    # Fit BKMR using MCMC
    fit = kmbayes(
      y = y_train,
      Z = exposures,
      X = covars,
      iter = 1000, # Change to 10000; run time was too long
      varsel = TRUE, # Variable selection,
      est.h = TRUE
    )
    
  }
  
  else if (model_type %in% c("BKMR2")){
    
    data = data %>% 
      distinct()
    
    exposures_and_covars = data %>% 
      dplyr::select(M1:C5) %>% 
      as.matrix()
    
    y_train = data %>% 
      dplyr::select(Y) %>% 
      as.matrix()
    
    # Use knots to improve speed
    knots100 = fields::cover.design(
      exposures_and_covars,
      nd = 100
    )$design
    
    # Fit BKMR using MCMC
    fit = kmbayes(
      y = y_train,
      Z = exposures_and_covars,
      iter = 1000, # Change to 10000; run time was too long
      varsel = TRUE, # Variable selection,
      est.h = TRUE
    )
    
  }
  
  return(fit)
  
}
```

```{r}
model_fnct = function(scenarios_count, datasets_count, metals_count, boot_count, model_type){
  
  estimates = data.frame()
  
  for (i in 1:scenarios_count){
    
    datasets = data.frame()
    
    filtered_datasets = dataset_filter(i)
    
    for (j in 1:datasets_count){
      
      data = filtered_datasets[[j]] %>% as.data.frame() %>% dplyr::select(M1:Y)
      
      model_fits = model_fit_fnct(model_type, data)
      
      boot_fits = boot_fnct(model_type, data, boot_count)
      
      metals = data.frame()
      
      for (k in 1:metals_count){
        
        if (model_type %in% c("linear", "GAM", "GAM2", "SuperLearner", "Quantile G-Computation", "AMC G-Computation", "BKMR1", "BKMR2")){
          
          # Set Q1 and Q2 for estimand
          q1 = metal_IQR_table(i, k)[1, ] %>% dplyr::select(M1:C5) # Consider reverting back to without curly brackets?
          q2 = metal_IQR_table(i, k)[2, ] %>% dplyr::select(M1:C5) # Consider reverting back to without curly brackets?
          
          # Set Q1 and Q1 for aggregate mixture profile
          q1_mix = metal_IQR_mix_table(i)[1, ] %>% dplyr::select(M1:C5)
          q2_mix = metal_IQR_mix_table(i)[2, ] %>% dplyr::select(M1:C5)
          
          # Apply fitted model to original data to estimate causal effect for individual metals and for mixture profile
          estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)[[1]]
          estimated_diff_mix = predict_diff_fnct(model_type, model_fits, q1_mix, q2_mix)[[1]]
          
          # For each bootstrap, predict effect estimate for individual metals and for mixture profile
          boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
          boot_estimated_diffs_mix = predict_diff_boots(model_type, boot_fits, q1_mix, q2_mix)
          
        }
        
        else if (model_type == "elastic net"){
            
            # Set Q1 and Q2 for estimand
            q1 = metal_IQR_table(i, k)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
            q2 = metal_IQR_table(i, k)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
            
            # Set Q1 and Q1 for aggregate mixture profile
            q1_mix = metal_IQR_mix_table(i)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
            q2_mix = metal_IQR_mix_table(i)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
                
            # Apply fitted model to original data to estimate causal effect
            estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)
            estimated_diff = estimated_diff[length(estimated_diff)] # Take only last prediction at point of convergence
            estimated_diff_mix = predict_diff_fnct(model_type, model_fits, q1_mix, q2_mix)
            estimated_diff_mix = estimated_diff_mix[length(estimated_diff_mix)]
            
            # For each bootstrap, predict effect estimate
            boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
            boot_estimated_diffs_mix = predict_diff_boots(model_type, boot_fits, q1_mix, q2_mix)
            
        }
          
        else if (model_type %in% c("MARS", "Random Forest", "Causal Forest")){
          
          # Set Q1 and Q2 for estimand
          q1 = metal_IQR_table(i, k)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          q2 = metal_IQR_table(i, k)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          
          # Set Q1 and Q1 for aggregate mixture profile
          q1_mix = metal_IQR_mix_table(i)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          q2_mix = metal_IQR_mix_table(i)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          
          # Apply fitted model to original data to estimate causal effect
          estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)[[1]]
          estimated_diff_mix = predict_diff_fnct(model_type, model_fits, q1_mix, q2_mix)[[1]]
            
          # For each bootstrap, predict effect estimate
          boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
          boot_estimated_diffs_mix = predict_diff_boots(model_type, boot_fits, q1_mix, q2_mix)
          
        }
          
        else if (model_type == "BART"){
          
          # Set Q1 and Q2 for estimand
          q1 = metal_IQR_table(i, k)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          q2 = metal_IQR_table(i, k)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          
          # Set Q1 and Q1 for aggregate mixture profile
          q1_mix = metal_IQR_mix_table(i)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          q2_mix = metal_IQR_mix_table(i)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          
          # Apply fitted model to original data to estimate causal effect
          # Using median difference for BART; is there a preferable method, e.g. yhat.train.mean or yhat.test.mean?
          estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)[[1]]
          estimated_diff_mix = predict_diff_fnct(model_type, model_fits, q1_mix, q2_mix)[[1]]
          
          # For each bootstrap, predict effect estimate
          boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
          boot_estimated_diffs_mix = predict_diff_boots(model_type, boot_fits, q1_mix, q2_mix)
          
        }
        
        # Find quantiles for confidence intervals
        ci_ll = quantile(boot_estimated_diffs %>% as.matrix(), 0.025)
        ci_ul = quantile(boot_estimated_diffs %>% as.matrix(), 0.975)
        ci_ll_mix = quantile(boot_estimated_diffs_mix %>% as.matrix(), 0.025)
        ci_ul_mix = quantile(boot_estimated_diffs_mix %>% as.matrix(), 0.975)
        
        row = bind_cols(
          scenario = i,
          dataset = j,
          metal = k,
          point_estimate = estimated_diff,
          CI_lower = ci_ll,
          CI_upper = ci_ul
        )
        
        metals = bind_rows(metals, row)
  
        }
      
      row_mix = bind_cols(
      scenario = i,
      dataset = j,
      metal = 11, # Must be numeric; consider metal 11 as aggregate mixture
      point_estimate = estimated_diff_mix,
      CI_lower = ci_ll_mix,
      CI_upper = ci_ul_mix
    )
    
      metals = bind_rows(metals, row_mix)  
      datasets = bind_rows(datasets, metals)
      
    }
    
    estimates = bind_rows(estimates, datasets)
    
  }
  
  model_performance = cbind(performance(estimates), model_type)
  return(model_performance)
  
}
```

```{r}
# Testing model performance on one metal from one dataset from one scenario with one bootstrap

# Linear
set.seed(2132)
t1 = Sys.time()
linear_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "linear")
t2 = Sys.time()
linear_time = t2 - t1

# Elastic net
set.seed(2132)
t3 = Sys.time()
elastic_net_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "elastic net")
t4 = Sys.time()
elastic_net_time = t4 - t3

# MARS
set.seed(2132)
t5 = Sys.time()
mars_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "MARS")
t6 = Sys.time()
mars_time = t6 - t5

# GAM (with caret)
set.seed(2132)
t7 = Sys.time()
gam_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "GAM")
t8 = Sys.time()
gam_time = t8 - t7

# BART
set.seed(2132)
t9 = Sys.time()
bart_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "BART")
t10 = Sys.time()
bart_time = t10 - t9

# Random forest (ranger)
set.seed(2132)
t11 = Sys.time()
random_forest_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "Random Forest")
t12 = Sys.time()
random_forest_time = t12 - t11

# GAM2 (without caret)
set.seed(2132)
t13 = Sys.time()
gam2_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "GAM2")
t14 = Sys.time()
gam2_time = t14 - t13

# SuperLearner
set.seed(2132)
t15 = Sys.time()
superlearner_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "SuperLearner")
t16 = Sys.time()
superlearner_time = t16 - t15

# Quantile G-Computation
set.seed(2132)
t17 = Sys.time()
qgcomp_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "Quantile G-Computation")
t18 = Sys.time()
qgcomp_time = t18 - t17

# Causal Forest
set.seed(2132)
t19 = Sys.time()
causal_forest_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "Causal Forest")
t20 = Sys.time()
causal_forest_time = t20 - t19

# AMC-based G-Computation
# set.seed(2132)
# t21 = Sys.time()
# amc_gcomp_df = model_fnct(scenarios_count = 2, datasets_count = 2, metals_count = 2, boot_count = 2, model_type = "AMC G-Computation")
# t22 = Sys.time()
# amc_gcomp_time = t22 - t21

# BKMR (confounders outside of kernel)
# set.seed(2132)
# t23 = Sys.time()
# bkmr1_df = model_fnct(scenarios_count = 1, datasets_count = 1, metals_count = 1, boot_count = 1, model_type = "BKMR1")
# t24 = Sys.time()
# bkmr1_time = t24 - t23

# BKMR (confounders inside of kernel)
# set.seed(2132)
# t25 = Sys.time()
# bkmr2_df = model_fnct(scenarios_count = 1, datasets_count = 1, metals_count = 1, boot_count = 1, model_type = "BKMR2")
# t26 = Sys.time()
# bkmr2_time = t26 - t25

# Add BKMR later
all_models_df = rbind(linear_df, elastic_net_df, mars_df, gam_df, bart_df, random_forest_df, gam2_df, superlearner_df, qgcomp_df, causal_forest_df)#, # amc_gcomp_df)
#, bkmr1_df, bkmr2_df)

times = cbind(model = c("Linear", "Elastic Net", "MARS", "GAM", "BART", "Random Forest", "GAM2", "SuperLearner", "Quantile G-Computation", "Causal Forest"), #, "AMC G-Computation"),
              #, "BKMR1", "BKMR2"), 
              time_secs = c(linear_time, elastic_net_time, mars_time, gam_time, bart_time, random_forest_time, gam2_time, superlearner_time, qgcomp_time, causal_forest_time)) %>% #, amc_gcomp_time)) %>% 
                #, bkmr1_time, bkmr2_time)) %>% 
  as.data.frame() %>% 
  mutate(
    time_secs = as.numeric(time_secs)
  ) %>% 
  mutate(
    predicted_times_hrs = time_secs/3600 * 1000 * 400 # 1000 boots per dataset & 400 datasets per scenario
  )

# Test graphics function
perform_graphics(model_name = "linear", all_models_df) # Change to all_models_df once all models are re-run
```

```{r}
# Check for any preliminary relative bias outliers
all_models_df %>% 
  dplyr::select(scenario, metal, relative_bias, model_type) %>%
  filter(metal == 1) %>% 
  ggplot(aes(x = metal, y = relative_bias, color = model_type)) + 
  geom_point() + 
  facet_grid(. ~ scenario)
```

```{r}
# Test aggregate mixture profiles (metal 11)
set.seed(2132)
model_fnct(scenarios_count = 5, datasets_count = 5, metals_count = 5, boot_count = 5, model_type = "linear")
model_fnct(scenarios_count = 5, datasets_count = 5, metals_count = 5, boot_count = 5, model_type = "elastic net")
```

