---
title: "Simulation Study for Biostatistics Practicum"
author: 'Zachary Katz (UNI: zak2132)'
date: "2023-01-26"
output: html_document
---

```{r}
# Load packages
library(tidyverse)
library(MASS)
library(corrplot)
library(faux)
library(glmnet)
library(rsample)
library(caret)
library(earth)
library(BART)
library(ranger)
library(mgcv)
library(SuperLearner)
library(qgcomp)
library(grf)
library(amc)
# library(bkmr)
library(patchwork)
```

# Simulate data

```{r}
# Data simulation function

data_simulation = function(scenario, N = 1000) {
  
  # Scenario 1: baseline (simple exposures, simple confounding)
  if (scenario == 1){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate linear effect from two exposures (M1, M5) and five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
      0.25*z[ind1] + 0.25*z[ind2] + 0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7]
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }

  # Scenario 2: complex exposures (nonlinear / interactions), simple confounding
  if (scenario == 2) {

    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 3: complex exposures (nonlinear / interactions) + multicollinearity, simple confounding
  if (scenario == 3){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # Exposures have higher correlation to each other
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 4: complex exposures (trigonometric), simple confounding
  if (scenario == 4){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    ) 
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 5: complex exposures (trigonometric) + multicollinearity, simple confounding
  if (scenario == 5){
  
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # Exposures have higher correlation to each other
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    sim_df = data.frame(correlated_vars)
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear and interactive effect from two exposures (M1, M5) and linear effect from five confounders (C1, C2, C3, C4, C5)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
      0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
      0.1*z[ind3] + 0.1*z[ind4] + 0.1*z[ind5] + 0.1*z[ind6] + 0.1*z[ind7] # Simple confounding effect
      
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 6: simple exposures, complex confounding
  if (scenario == 6){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate linear effect from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1-5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
  
      0.5*z[ind1] + 0.5*z[ind2] + # Simple exposure effect, omitting C2
      0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
      0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
}
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenarion 7: complex exposures (nonlinear / interactions), complex confounding
  if (scenario == 7){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect (nonlinear/interaction)
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 8: complex exposures (trigonometric), complex confounding
  if (scenario == 8){
  
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = 0.25,
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # Scenario 9: complex exposures (nonlinear / interactions) + multicollinearity, complex confounding
  if (scenario == 9){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # High collinearity among the exposures
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.2*(z[ind1])^2 + 0.2*(z[ind2])^2 + 0.1*(z[ind1])*(z[ind2]) + # Complex exposure effect (nonlinear/interaction)
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }
  
  # 10 = complex exposures (trigonometric) + multicollinearity, complex confounding
  if (scenario == 10){
    
    # Simulate 15 normally distributed variables: 10 exposures and 5 confounders
    # High collinearity among the exposures
    correlated_vars = rnorm_multi(
      n = N,
      mu = rep(x = 1, 15),
      sd = rep(x = 0.2, 15),
      r = c(1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.7, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 1, 0.25, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1, 0.25,
            0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 1),
      varnames = c("M1", "M2", "M3", "M4", "M5", "M6", "M7", "M8", "M9", "M10", "C1", "C2", "C3", "C4", "C5"),
      empirical = FALSE
    )
    
    # Confound M1 and M2 with more complexity than before
    M1 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 1],
          sd = 0.2
        )
  
    M2 = rnorm(
          N, 
          mean = sin(0.2*correlated_vars[, 11] + 0.2*correlated_vars[, 12] + 0.2*correlated_vars[, 13] +
                    0.2*correlated_vars[, 14] + 0.2*correlated_vars[, 15]) + 0.8*correlated_vars[, 2],
          sd = 0.2
        )  
    
    sim_df = data.frame(M1, M2, correlated_vars[, 3:15])
    colnames(sim_df[, 1:10]) = paste("exposure_", 1:10, sep = "") 
    colnames(sim_df[, 11:15]) = paste("confounder_", 1:5, sep = "")
    
    # Simulate nonlinear effect with interactions from two exposures (M1, M5) and nonlinear effect with interactions from five confounders (C1, C2, C3, C4, C5)
    # M1 is confounded with C1-5, M5 is not
    # M2 is correlated with C1-5, but has no direct effect on outcome (spurious association)
    hfunY = function(z) h_fun_Y(z, ind1 = 1, ind2 = 5, ind3 = 11, ind4 = 12, ind5 = 13, ind6 = 14, ind7 = 15) 
    h_fun_Y = function(z, ind1, ind2, ind3, ind4, ind5, ind6, ind7) {
      
        0.35*sin(z[ind1]) + 0.35*sin(z[ind2]) + 0.3*(z[ind1])*(z[ind2]) + # Complex exposure effect using sin
        0.2*(z[ind3])^2 + 0.2*(z[ind4])^2 + 0.2*(z[ind5])^2 + 0.1*z[ind6] + 0.1*z[ind7] + # Quadratic confounding effect from C1-3
        0.1*(z[ind6])*(z[ind7]) # Interactive confounding effect from C4, C5
    }
    
    sim_df$hY = apply(sim_df, 1, hfunY) 
    
    # Add noise
    epsY = rnorm(N, mean = 1, sd = 0.2)
    sim_df$Y = with(sim_df, hY + epsY)
    
    # Remove unused columns
    sim_df = sim_df %>% 
      dplyr::select(-hY)
    
    # Return simulated data
    return(sim_df)
    
  }

}

# Check correlations
data = data_simulation(scenario = 10, N = 1000)
corr = cor(data)
corrplot(corr, type = "upper")
```

```{r}
# Function for functional forms

functional_forms = function(scenario, covars){
  
  if(scenario == 1){
    
    a = 0.25*covars[1] + 0.25*covars[5] + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
  }
  
  if(scenario %in% c(2, 3)){
    
    a = 0.2*(covars[1])^2 + 0.2*(covars[5])^2 + 0.1*(covars[1]*covars[5]) + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
  }
  
  if(scenario %in% c(4, 5)){
    
    a = 0.35*sin(covars[1]) + 0.35*sin(covars[5]) + 0.3*(covars[1])*(covars[5]) + 0.1*covars[11] + 0.1*covars[12] + 0.1*covars[13] + 0.1*covars[14] + 0.1*covars[15]
    
      
  }
  
  if (scenario == 6){
    
    a = 0.5*covars[1] + 0.5*covars[5] + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])
    
    }
    
  if (scenario %in% c(7, 9)){
    
    a = 0.2*(covars[1])^2 + 0.2*(covars[5])^2 + 0.1*(covars[1]*covars[5]) + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])
      
    }
    
  if (scenario %in% c(8, 10)){
    
    a = 0.35*sin(covars[1]) + 0.35*sin(covars[5]) + 0.3*(covars[1])*(covars[5]) + 0.2*(covars[11])^2 + 0.2*(covars[12])^2 + 0.2*(covars[13])^2 + 0.1*covars[14] + 0.1*covars[14] + 0.1*(covars[14])*(covars[15])

    }
 
  return(a)   
}
```

```{r}
# First estimand of interest: effect of interquartile range change in exposure levels

quantiles = data.frame()

# For each scenario, simulate 100 datasets with 1000 observations

for (i in 1:10){
  
  for (j in 1:100){
    
    sim_data = data_simulation(scenario = i, N = 1000)
    q1 = apply(sim_data, 2, function(x) quantile(x, 0.25))
    med = apply(sim_data, 2, median)
    q2 = apply(sim_data, 2, function(x) quantile(x, 0.75))
    quantiles = bind_rows(
      quantiles,
      bind_cols(scenario = i, quantile = c(0.25, 0.50, 0.75),
      bind_rows(q1, med, q2)
      )
    )
    
  }
  
}

# For each scenario, average across datasets for each quantile of a given metal

# Function for table
metal_IQR_table = function(scenario, metal){
  
  # Average across datasets of each quantile for given metal in given scenario
  m.25 = quantiles %>% 
          filter(quantile == 0.25 & scenario == {{scenario}}) %>% 
          dplyr::select(c(metal + 2)) %>% 
          t() %>% 
          mean()
    
  m.75 = quantiles %>% 
          filter(quantile == 0.75 & scenario == {{scenario}}) %>% 
          dplyr::select(c(metal + 2)) %>% 
          t() %>% 
          mean()
  
  # For interquartile range change
  # Create tables with all vars at median except for given exposure to obtain potential outcomes (counterfactuals)
  q1.m = apply(filter(quantiles, quantile == 0.5 & scenario == {{scenario}}), 2, mean)
  q1.m[[c(metal + 2)]] = m.25
  q2.m = q1.m
  q2.m[[c(metal + 2)]] = m.75
  
  table = rbind(q1.m, q2.m) %>% as.data.frame() %>% dplyr::select(-scenario, -quantile)
  
  return(table)
  
}

# Function for causal effect (difference in potential outcomes)
metal_IQR_effect = function(scenario, metal){
  
  # Obtain metal IQR table
  table = metal_IQR_table(scenario, metal)
  
  # Calculate true effect
  true_effect = functional_forms(scenario, table[2, ]) - functional_forms(scenario, table[1, ])
  
  return(true_effect[[1]])
  
}
```

```{r}
# Calculate true effects

true_effects = data.frame()

for (scenario in 1:10){
  
  for (metal in 1:10){
    
    effect = metal_IQR_effect(scenario, metal)
    true_effects = bind_rows(
      true_effects,
      bind_cols(scenario = scenario,
                metal = metal,
                effect = effect)
    )
    
  }
  
}

true_effects = true_effects %>% 
  pivot_wider(
    names_from = metal,
    values_from = effect
  )
```

```{r}
# Simulate M data sets of N observations for each scenario
set.seed(2132)
M = 10 # number of data sets per scenario (10 for testing code; 100 or 1000 eventually)
N = 1000 # number of observations per data set

simulated_data = list()

for (scenario in 1:10){
  
  simulated_data_loop = lapply(1:M, function(i) data_simulation(scenario = scenario, N = N))
  
  for (data_frame in 1:M){
    
    simulated_data_loop[[data_frame]]$scenario = scenario
    
  }
  
  simulated_data = append(simulated_data, simulated_data_loop)
  
}
```

# Other functions

```{r}
# Function to bootstrap N times on a given model
boot_fnct = function(model_type, bootstrap_data, boot_count){
  
  set.seed(2132)
  
  boot = bootstrap_data %>% 
    dplyr::select(M1:Y) %>%
    bootstraps(times = boot_count) %>% 
    rowwise() %>% 
    mutate(data_sample = (list(analysis(splits)))) %>% 
    dplyr::select(id, data_sample) %>% 
    ungroup() %>%
    mutate(models = map(data_sample, model_type = model_type, model_fit_fnct)) %>% 
    dplyr::select(-data_sample)
  
  return(boot)
  
}

# Function to estimate causal effect using fitted model

predict_diff_fnct = function(model_type, fitted_model, q1, q2){
  
  if (model_type %in% c("linear","elastic net", "MARS")){
    
    predicted = predict(fitted_model, q2) - predict(fitted_model, q1)
    
    return(predicted)
    
  }
  
}

# predict_diff_fnct = function(fitted_model){
#   
#   effect = ifelse(
#     model == "BART",
#     # For BART, SE medians from MC draws; is there a better way?
#     quantile(predict(fitted_model, q2), 0.5) - quantile(predict(fitted_model, q1), 0.5),
#     ifelse(
#       model == "GAM_MGCV",
#       predict(fitted_model, q2 %>% as.data.frame()) - predict(fitted_model, q1 %>% as.data.frame()),
#       ifelse(
#         model == "SuperLearner",
#         predict(fitted_model, q2, type = "response")$pred - predict(superlearner_fit, q1, type = "response")$pred,
#         ifelse(
#           model == "QuantileGComp" | model == "AMCGComp",
#           predict(fitted_model$fit, q2) - predict(fitted_model$fit, q1),
#           predict(fitted_model, q2) - predict(fitted_model, q1)
#         )
#       )
#     )
#   )
#   
#   return(effect)
#   
# }

# Function to estimate causal effects using fitted model on bootstraps
predict_diff_boots = function(model_type, boots, q1, q2){
  
  bstar = NULL 
  n = dim(boots)[1];
  
  for (mod in 1:n) {
    
    if (model_type == "elastic net"){
      
      quant1 = predict(boots[[2]][[mod]], q1)
      quant1 = quant1[length(quant1)] # Pull for smallest lambda
      quant2 = predict(boots[[2]][[mod]], q2)
      quant2 = quant2[length(quant2)] # Pull for smallest lambda
      diff = tibble(diff = quant2 - quant1)[[1]]
      
    }
    
    else if (model_type == "BART"){
      
      # Use medians; is there a better method?
      quant1 = quantile(predict(boots[[2]][[mod]], q1), 0.5)
      quant2 = quantile(predict(boots[[2]][[mod]], q2), 0.5)
      diff = tibble(diff = quant2 - quant1)[[1]]
      
    }
    
    else {
      
      diff = tibble(diff = predict_diff_fnct(
        model_type,
        fitted_model = boots[[2]][[mod]],
        q1,
        q2)[[1]]
    )
      
    }
    
    bstar = rbind(bstar, diff)
  } # Next draw
  
  return(bstar)
  
}
```

```{r}
# Function for performance metrics
performance_metrics = function(estimates, scenario, metal){
  
  true_effect = true_effects[scenario, metal + 1]
  
  estimates = estimates %>% 
    dplyr::select(-dataset) %>% 
    filter(scenario == {{scenario}} & metal == {{metal}}) %>% 
    mutate(
      true_effect = as.numeric(true_effect)
    ) %>% 
    mutate(
      bias = abs(true_effect - point_estimate),
      relative_bias = bias / true_effect,
      standard_error = (true_effect - point_estimate)^2,
      includes = if_else(CI_lower <= true_effect & CI_upper >= true_effect,
                         1,
                         0),
      CI_length = CI_upper - CI_lower,
      sig_effect = case_when(
        CI_lower > 0 & CI_upper > 0 ~ 1,
        CI_lower < 0 & CI_upper < 0 ~ 1,
        CI_lower < 0 & CI_upper > 0 ~ 0
      )
    )
  
  estimates_grouped = estimates %>% 
    group_by(scenario, metal) %>% 
    mutate(
      RMSE = sqrt(mean(standard_error)),
      coverage = mean(includes),
      TDR = dplyr::case_when(
        metal %in% c(1, 5) ~ mean(sig_effect, na.rm = TRUE)
        ),
      FDR = dplyr::case_when(
        !(metal %in% c(1, 5)) ~ mean(sig_effect, na.rm = TRUE)
      )
    )
  
  row = estimates_grouped %>% 
    dplyr::select(scenario, metal, RMSE, coverage, TDR, FDR) %>% 
    head(1)
  
  return(row)

}

# Loop across models, scenarios, metals
performance = function(estimates){
  
  performance_df = data.frame()

    for (scenario in 1:10){
      
      for (metal in 1:10){
        
        # Run performance function for linear model
        row = performance_metrics(estimates, scenario, metal)
        
        # Append to data frame
        performance_df = bind_rows(performance_df, row)
        
      }
  
    }
  
  return(performance_df)
  
}
```

```{r}
# Function to filter to correct dataset
dataset_filter = function(scenario){
  
  # Filter for scenario and then to each individual dataset for looping
  first_index = (scenario * M) - (M - 1) 
  last_index = first_index + (M - 1)
  indices = seq(first_index, last_index, by = 1)
  filtered_datasets = simulated_data[indices]
  
  return(filtered_datasets)
  
}
```

```{r}
# Performance graphics
perform_graphics = function(model_name, data = all_models_df){
  
  base = data %>% 
    filter(model_type == {{model_name}}) %>% 
    mutate(
      metal = as.factor(metal),
      scenario = as.factor(scenario)
    ) %>% 
    ggplot()
  
  a = base + 
    geom_point(aes(x = metal, y = RMSE, group = scenario, color = scenario))
  
  b = base + 
    geom_point(aes(x = metal, y = coverage, group = scenario, color = scenario))
  
  c = base + 
    geom_point(aes(x = metal, y = TDR, group = scenario, color = scenario))
  
  d = base + 
    geom_point(aes(x = metal, y = FDR, group = scenario, color = scenario))
  
  graphs = (a + b) / (c + d)
  
  return(graphs)
  
}
```

# Test models

```{r}
# Function to fit models
model_fit_fnct = function(model_type, data){
  
  if (model_type %in% c("linear")){
    
    fit = lm(Y ~ ., 
          data = data)
    
  }
  
  else if (model_type %in% c("elastic net")){
    
    x_data = data %>% 
      dplyr::select(-Y) %>% 
      as.matrix()
    
    y_data = data %>% 
      dplyr::select(Y) %>% 
      as.matrix()
  
    # Force covariates into model (specify covariates not to drop; 0 for vars to keep)
    p_fac = rep(1, dim(x_data)[2])
    p_fac[11:15] = 0 # Keep confounders in model
    
    # Vector of values identifying what fold each observation is in for 10-fold cross-validation
    fold_id = sample(x = 1:10, size = length(y_data), replace = TRUE)
    
    # Fit model to training data with cross-validation
    # Alpha = 1 by default
    cv_fit = cv.glmnet(
      x = x_data,
      y = y_data,
      family = "gaussian",
      maxit = 5000,
      penalty.factor = p_fac,
      foldid = fold_id 
    )
    
    # Need to hard code cross-validation over range of alpha values
    for (i in seq(0, 1, by = 0.1)){
      cv_temp = cv.glmnet(
        x = x_data,
        y = y_data,
        family = "gaussian",
        maxit = 5000,
        penalty.factor = p_fac,
        foldid = fold_id,
        alpha = i
      )
      
      if(min(cv_temp$cvm) < min(cv_fit$cvm)) 
        {cv_fit = cv_temp}
    
  }
  
  # Optimal beta coefficients
  selected_beta_coefs = array(t(as.matrix(coef(cv_fit,
                                         s = cv_fit$lambda.min))))
  
  # Final trained model for prediction
  # Returns elastic net model with coefficients across grid of values for regularization parameter lambda
  fit = glmnet(
    x = x_data,
    y = y_data,
    family = "gaussian",
    init = selected_beta_coefs,
    iter = 0
  )
    
  }
  
  else if (model_type %in% c("MARS")){
    
    train_data = data %>% 
      as.matrix()
  
    # Specify confounders to force into the model
    covars = c("C1", "C2", "C3", "C4", "C5") 
    
    # Implement 5-fold CV using caret package
    myControl = trainControl(
      method = "cv",
      number = 5,
      summaryFunction = defaultSummary
    )
    
    # Set hyperparameter tuning grid, may want to consider different values
    hyper_grid = expand.grid(
      degree = c(1,2), # Limit to up to second-order polynomials
      nprune = seq(5, 50, length.out = 10) %>% floor()
    )
    
    # Fit model
    fit = train(
      Y ~ .,
      data = train_data,
      method = "earth",
      linpreds = covars, # Enter covariates linearly
      thresh = 0, # Include a predictor even if it has very little predictive power
      penalty = -1, # Do not discard any terms in backwards pass
      trControl = myControl,
      tuneGrid = hyper_grid
    )
    
  }
  
  return(fit)
  
}
```

```{r}
model_fnct = function(scenarios_count, datasets_count, metals_count, boot_count, model_type){
  
  estimates = data.frame()
  
  for (scenario in 1:scenarios_count){
    
    datasets = data.frame()
    
    filtered_datasets = dataset_filter(scenario)
    
    for (dataset in 1:datasets_count){
      
      data = filtered_datasets[[dataset]] %>% as.data.frame() %>% dplyr::select(M1:Y)
      
      model_fits = model_fit_fnct(model_type, data)
      
      boot_fits = boot_fnct(model_type, data, boot_count)
      
      metals = data.frame()
      
      for (metal in 1:metals_count){
        
        if (model_type == "linear"){
          
          # Set Q1 and Q2 for estimand
          q1 = metal_IQR_table(scenario = scenario, metal = metal)[1, ] %>% dplyr::select(M1:C5)
          q2 = metal_IQR_table(scenario = scenario, metal = metal)[2, ] %>% dplyr::select(M1:C5)
          
          # Apply fitted model to original data to estimate causal effect
          estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)[[1]]
          
          # For each bootstrap, predict effect estimate
          boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
          
        }
        
      else if (model_type == "elastic net"){
          
          # Set Q1 and Q2 for estimand
          q1 = metal_IQR_table(scenario = scenario, metal = metal)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
          q2 = metal_IQR_table(scenario = scenario, metal = metal)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
              
          # Apply fitted model to original data to estimate causal effect
          estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)
          estimated_diff = estimated_diff[length(estimated_diff)] # Take only last prediction at point of convergence
          
          # For each bootstrap, predict effect estimate
          boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
          
      }
        
      else if (model_type == "MARS"){
        
        # Set Q1 and Q2 for estimand
        q1 = metal_IQR_table(scenario = scenario, metal = metal)[1, ] %>% dplyr::select(M1:C5) %>% as.matrix()
        q2 = metal_IQR_table(scenario = scenario, metal = metal)[2, ] %>% dplyr::select(M1:C5) %>% as.matrix()
        
        # Apply fitted model to original data to estimate causal effect
        estimated_diff = predict_diff_fnct(model_type, model_fits, q1, q2)[[1]]
          
        # For each bootstrap, predict effect estimate
        boot_estimated_diffs = predict_diff_boots(model_type, boot_fits, q1, q2)
        
      }
        
      # Find quantiles for confidence intervals
      ci_ll = quantile(boot_estimated_diffs %>% as.matrix(), 0.025)
      ci_ul = quantile(boot_estimated_diffs %>% as.matrix(), 0.975)
      
      row = bind_cols(
        scenario = scenario,
        dataset = dataset,
        metal = metal,
        point_estimate = estimated_diff,
        CI_lower = ci_ll,
        CI_upper = ci_ul
      )
      
      metals = bind_rows(metals, row)

      }
      
    datasets = bind_rows(datasets, metals)
      
    }
    
    estimates = bind_rows(estimates, datasets)
    
  }
  
  model_performance = cbind(performance(estimates), model_type)
  return(model_performance)
  
}
```

```{r}
# Testing model performance
linear_df = model_fnct(scenarios_count = 10, datasets_count = M, metals_count = 10, boot_count = 100, model_type = "linear")

elastic_net_df = model_fnct(scenarios_count = 10, datasets_count = M, metals_count = 10, boot_count = 100, model_type = "elastic net")

mars_df = model_fnct(scenarios_count = 10, datasets_count = M, metals_count = 10, boot_count = 100, model_type = "MARS")

all_models_df = rbind(linear_df, elastic_net_df, mars_df)

# Test graphics function
perform_graphics(model_name = "MARS", all_models_df)
```